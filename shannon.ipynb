{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy, as introduced by Shannon in his “A Mathematical Theory of Communication”, is a measure of the amount of uncertainty or randomness in a message. It is defined as the negative sum of the probabilities of each symbol multiplied by the logarithm of that probability.\n",
    "\n",
    "Shannon’s entropy provides a theoretical limit on the amount of information that can be transmitted over a channel, as well as on the best possible data compression that can be achieved.\n",
    "\n",
    "In general, the more different letters in a message, and the longer the message, the more unpredictable the message is and the more information it contains. This is because a message with a greater variety of symbols has more possible outcomes and therefore, carries more information.\n",
    "\n",
    "On the other hand, a message with fewer letters and shorter length is more predictable and contains less information, as there are fewer possible outcomes. This is reflected in the entropy, as a message with a greater variety of symbols will have a higher entropy and a message with fewer symbols will have a lower entropy.\n",
    "\n",
    "Recommended: plot the entropy function according to different texts, lengths and languages.\n",
    "\n",
    "Analysis: If the entropy of dissipative system tends to decrease, the emergence patterns are somehow related to the minimisation of the entropy function that best represents the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of the message is:  3.8954201887252182\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "def entropy(message):\n",
    "    # Create a dictionary to store the frequency of each letter\n",
    "    letter_freq = {}\n",
    "    for letter in message:\n",
    "        if letter.isalpha():\n",
    "            if letter in letter_freq:\n",
    "                letter_freq[letter] += 1\n",
    "            else:\n",
    "                letter_freq[letter] = 1\n",
    "   \n",
    "    # Calculate the probability of each letter\n",
    "    total_letters = sum(letter_freq.values())\n",
    "    prob = {letter: freq/total_letters for letter, freq in letter_freq.items()}\n",
    "   \n",
    "    # Calculate the entropy\n",
    "    entropy = 0\n",
    "    for letter in prob:\n",
    "        entropy += prob[letter] * log2(prob[letter])\n",
    "   \n",
    "    return -entropy\n",
    "\n",
    "message2 = \"On the other hand, a message with fewer letters and shorter length is more predictable and contains less information\"\n",
    "message1 = \"Unfortunately Unfortunately Unfortunately Unfortunately Unfortunately \"\n",
    "message = \"uuabcuuuuuuuuuu\"\n",
    "\n",
    "print(\"The entropy of the message is: \", entropy(message2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83274c31c9221182fb1fe126cd0ae34f8001af217d374c7998aef38dd4f692b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
